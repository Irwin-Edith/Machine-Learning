由于最近毕设要用到transformer，但是机器学习已经忘的差不多了，所以我索性就复习一下，然后写点东西。
# Transformer的基本架构
主要是由编码器（Encoder）和解码器（Decoder）组成，每个部分有多个层（Layer）构成。
## 输入嵌入（Input Embedding）
输入的数据通过嵌入层（Embedding）转化为向量表示，然后与位置编码（Position Embedding）相加。位置编码是告诉模型输入数据中各个元素的顺序信息，因为Transformer本身并不处理序列的顺序。
## 编码器（Encoder）
编码器是由多个相同的层组成的，每个曾包括两个部分：
### 自注意力机制（Self-Attention）
这个部分是Trans的核心。通过计算与其他单词/或元素的关系来调整其表示。自注意力的计算主要分为以下步骤：
1. 对输入序列进行线性变换，得到三个向量：查询（Query）、键（Key）、值（Value）。
2. 通过查询与键的点击（Q·K）来计算注意力分数，决定哪些值对当前元素的重要性较大。
3. 对所有值的加权求和得到新的表示。
### 前馈神经网络
每一层的输出经过自注意力机制后，会传入一个全连接的前馈神经网络，这个网络通常包括两个线性层和一个激活函数（ReLU）。
每一层还会应用**层归一化**和**残差连接**。有助于梯度的传播和稳定性。
## 解码器（Decoder）
解码器和编码器结构类似，但它还包括一个额外的编码器-解码器注意力机制（Encoder-Decoder Attention），用于通过编码器的输出调整解码器的表示。
## 输出层
解码器的最后输出通过一个线性变换映射到词汇表大小的维度上，然后应用softmax激活函数，输出最终的预测概率分布。

---

# Attention机制的详细介绍
自注意力机制（Self-Attention）是Transformer中最核心的部分。假设我们有一个输入序列，每个元素都可以表示为一个向量。Attention机制通过计算每个元素与其他元素的关系（即注意力权重）来调整它们的表示。
1. 查询（Query）：用于查找相关信息的向量。
2. 键（Key）：每个元素的标识符，帮助找到相关信息。
3. 值（Value）：包含具体信息的向量，将被加权组合以产生最终的输出。
>具体计算方式是：
计算查询向量与键向量的点积，得到一个注意力得分。
使用softmax函数对注意力得分进行归一化，得到每个元素的权重。
用这些权重加权平均值向量，得到该位置的输出。
# 多头注意力（Multi-Head Attention）
在实际的Transformer中，会使用多个“头”（Head）来并行计算不同的注意力分数，每个头学习到不同的表示。多个头的结果会被拼接（concatenate）起来，然后通过一个线性层处理，形成最终的输出。